# -*- coding: utf-8 -*-
"""Ex3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fknESJxUqFgt1Bw1FQ9Xna_cOEGpsgGm
"""

import json
from transformers import AutoTokenizer, AutoModelForTokenClassification
import torch
from sklearn.metrics import f1_score
import requests
import gdown
import numpy as np
from sklearn.cluster import AgglomerativeClustering
from sklearn.metrics.pairwise import cosine_similarity

#Downloading the dataset from Drive and limiting it to the first 50 documents
file_id = "1e06-6hB4utzRujxWCPVlfB6LnT15EKQ4"
url = f"https://drive.google.com/uc?id={file_id}"
output_path = "/content/test.jsonl"
gdown.download(url, output_path, quiet=False)
def load_dataset(filepath, limit=50):
    data = []
    with open(filepath, 'r') as file:
        for i, line in enumerate(file):
            if i >= limit:
                break
            data.append(json.loads(line))
    return data
test_data = load_dataset(output_path)

# To see the size of the dataset
dataset_size = len(test_data)
print(f"The size of the dataset is: {dataset_size} documents")

# CorefBERT Model and Tokenizer
tokenizer_corefbert = AutoTokenizer.from_pretrained("nielsr/coref-bert-base")
model_corefbert = AutoModelForTokenClassification.from_pretrained("nielsr/coref-bert-base")

def corefbert_coref_resolution(text):
    tokens = tokenizer_corefbert(text, return_tensors="pt", truncation=True, padding=True, max_length=512)
    outputs = model_corefbert(**tokens)
    return outputs

# SpanBERT Model and Tokenizer
tokenizer_spanbert = AutoTokenizer.from_pretrained("SpanBERT/spanbert-large-cased")
model_spanbert = AutoModelForTokenClassification.from_pretrained("SpanBERT/spanbert-large-cased")

def spanbert_coref_resolution(text):
    inputs = tokenizer_spanbert(text, return_tensors="pt", truncation=True, padding=True, max_length=512)
    with torch.no_grad():
        outputs = model_spanbert(**inputs)
    return outputs

"""I tried to split the text to speed the process however the sessions collapsed as the process exceeds RAM."""

def split_text(text, max_length=512):
    tokens = tokenizer_corefbert.tokenize(text)
    chunks = [tokens[i:i + max_length] for i in range(0, len(tokens), max_length)]
    return [tokenizer_corefbert.convert_tokens_to_string(chunk) for chunk in chunks]

def process_dataset_in_batches(data, batch_size=4):
    results = {'corefbert': [], 'spanbert': []}

    for i in range(0, len(data), batch_size):
        batch = data[i:i + batch_size]

        for doc in batch:
            text = " ".join([" ".join(sentence) for sentence in doc['sentences']])

            text_chunks = split_text(text)

            corefbert_output = [corefbert_coref_resolution(chunk) for chunk in text_chunks]
            spanbert_output = [spanbert_coref_resolution(chunk) for chunk in text_chunks]

            results['corefbert'].append(corefbert_output)
            results['spanbert'].append(spanbert_output)

    return results

batch_size = 4
results = process_dataset_in_batches(test_data, batch_size)

print(results)